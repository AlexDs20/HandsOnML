% Specifies the type of document you have.
\documentclass{article}
\usepackage{array}
\usepackage{listings}
\usepackage{geometry}
\renewcommand{\arraystretch}{1.5}
\geometry{
  a4paper,
  total={140mm, 217mm}
}
%------------------------------

% Default fixed font does not support bold face
\DeclareFixedFont{\ttb}{T1}{txtt}{bx}{n}{10} % for bold
\DeclareFixedFont{\ttm}{T1}{txtt}{m}{n}{10}  % for normal

% Custom colors
\usepackage{color}
\definecolor{deepblue}{rgb}{0,0,0.5}
\definecolor{deepred}{rgb}{0.6,0,0}
\definecolor{deepgreen}{rgb}{0,0.5,0}

% Python style for highlighting
\newcommand\pythonstyle{\lstset{
language=Python,
basicstyle=\ttm,
otherkeywords={self},             % Add keywords here
keywordstyle=\ttb\color{deepblue},
emph={MyClass,__init__},          % Custom highlighting
emphstyle=\ttb\color{deepred},    % Custom highlighting style
stringstyle=\color{deepgreen},
showstringspaces=false            %
}}

% Python environment
\lstnewenvironment{python}[1][]
{
\pythonstyle
\lstset{#1}
}
{}

% Python for external files
\newcommand\pythonexternal[2][]{{
\pythonstyle
\lstinputlisting[#1]{#2}}}

% Python for inline
\newcommand\pythoninline[1]{{\pythonstyle\lstinline!#1!}}

% No indent
\setlength\parindent{0pt}
%------------------------------

\title{Notes on:\\ "Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow"}
\author{Alexandre De~Spiegeleer}

\begin{document}
% Define some things for hte python codes


\maketitle
\newpage

%----------------------------------------
\section{The machine learning landscape}
%----------------------------------------
ML is nothing more than fitting a model to the known data!
\subsection{Sturcture of a project}
\begin{enumerate}
    \item study the problem
    \item Train the ML algo (with lots of data)
    \item Solution
    \item Check the solution and better understand the problem
    \item[$\rightarrow$] try to improve model from the observed possible problems
\end{enumerate}

Examples:
\begin{itemize}
    \item Detection of signals in images using CNNs
    \item Classifying articles: NLP
\end{itemize}


\subsection{Types of ML}
Different categories of ML algorithms:

\begin{tabular}{p{0.20\textwidth}p{0.40\textwidth}p{0.4\textwidth}}
  \textbf{Properties} & \textbf{Descrition} & \textbf{Examples} \\
  \hline
  Supervised &  Need laballed training data & kNN, Linear Reg., Logistic Reg., SVM, Decision Tree, Random Forest, NN\ldots \\
  Unsupervised & No labelled data, the algorithm puts similar data together & Clustering: k-means, DBSCAN, HCA. Dimensional reduction: PCA, kernel PCA, LLE, t-SNE. Anomaly detection: One-class SVM, Isolation Forest\\
  Semi-Supervised & Both labelled and not labelled data & Deepd belief network, restricted Boltzman machine \\
  Reinforcment Learning & In the learning process give rewards or penalites depending on the action done & \\
  \hline\hline
  Batch Learning & Trains using ALL data $\rightarrow$ If needs to retrain, need to train on all the data again & \\
  Online Learning & Train by mini-batches: Train incrementally and can start again from a previous minibatch run & \\
  \hline\hline
  Instance Based & On new data, check distance to known data and assign same output &\\
  Model-based & Use the data to make predictions / interpolates to new data & \\
\end{tabular}

\subsubsection{Unsupervised Learning}
\begin{description}
    \item[Feature Extraction] Combines related features into a better one (e.g. using PCA)
    \item[Anomaly Detection] Find anomalies in dataset(e.g.\ removing some outliers)
    \item[Association rule learning] Discover relationships in large datasets
\end{description}

\subsubsection{semi-supervised Learning}
Often much data but only a few are labelled
\begin{itemize}
  \item[$\rightarrow$] Often combinaison of supervised and unsupervised algo.
\end{itemize}

\subsubsection{Reinforcement Learning}
Trains an agent by giving rewards and penalties to obtained the best policy

\subsection{Challenges}
\subsubsection{data}
\begin{description}
  \item[Lack of data] ML requires lots of data (mininmum thousands of examples).\\
                      Note that models performances increase with number of data.\\
                      $\rightarrow$ Choice to work on model or gather more data!

  \item[Non representative data] Data used for training must include similar data to those for predictions. (Poor extrapolation capacity)

  \item[Sampling Bias] non representative data because the sampling is flawed.\\
          Bias can appear if the proportion of data in different classes are not representative.

  \item[Poor data quality] if errors, outliers and noise in data

  \item[Irrelevant features] There should be enough relevant features and not much crap.
\end{description}


\subsubsection{Fitting}
\begin{description}
  \item[Overfitting] The model fits too well the training data. Occurs because model too complex compared to data $\rightarrow$ lose predictability.
  \item[Underfitting] Model too simple compared to the data to be represented e.g.\ Linear fit on a non-linear problem
\end{description}

Solutions to overfitting:
\begin{itemize}
  \item \textit{Regularization}\\
    $\rightarrow$ Making the model simpler to avoid by adjusting hyperparameter
  \item \textit{Hyperparameter}\\
    $\rightarrow$ Parameters of the learning algorithm that dictates the amount of regularization.
\end{itemize}

Solutions to underfitting
\begin{itemize}
  \item Select a better model
  \item Have better features
  \item reduce the constraints on the model (e.g.\ hyperparameters)
\end{itemize}

\subsection{Evaluating performance}

Split the data into training set (80\%) and testing set (20\%).
Evaluation on the test set gives an estimate of the error on unseen data.
When training multiple models on the training set and testing them on the test sets, our selection of the "best" model is which model best fits the test data set.
$\rightarrow$ Model cannot necessarily be good on new data.
Thus, keep a validation set.

Thus, there are 3 sets of data: Training, Validation and Test
\begin{enumerate}
  \item Train multiple models with different Hyperparameter on the training set
  \item Select the model that performs best on the validation set.
  \item Verify that the model is indeed good on the test set
  \item Deploy
\end{enumerate}

It is common to split the training set to train several models on sub-sets of the training set and train one final model on the whole training set.

If the validation set is too small $\rightarrow$ imprecise evaluation of performances.
The validation set cannot be too large either (compared to training set).\\
$\rightarrow$ use \emph{cross-validation}: it uses several small validation sets.
Each model is evaluated once per validation set.
You can then average the results of the model on the different smaller validation sets.

\subsection{Data mismatch}
If there are two types of data in the training set (e.g.\ not from the same source).
This may cause problem in the predictions.
How to know if overfitting or problem with the data?

Hold part of the training set (data from one of the source) and see how the model performs on that.
If it performs well $\rightarrow$ the error comes from the mismatched data
If it performs poorly $\rightarrow$ the error comes from the overfitting

If it is because of the data mismatched, is it possible to preprocess these to be more like the rest of the data?



%----------------------------------------
\newpage
\section{End-to-end machine learning project}
%----------------------------------------

Main steps of a machine learning project:
\begin{enumerate}
    \item Look at the big picture
    \item Get the data
    \item Discover and visualise the data
    \item Prepare the data for machine learning algorithms
    \item Select a model and train it
    \item Fine-tune the model
    \item Present the solution
    \item Launch, monitor and maintain the system
\end{enumerate}

\subsection{Look at the big picture}
Get info on the problem to be solved e.g.\
\begin{itemize}
    \item Objectives?
    \item input and output of the model? (i.e.\ features and regression/categories?)
    \item Overall project's pipeline
    \item Current status and precision $\rightarrow$ idea for the aimed accuracy
    \item Evaluate what the model needs: multiple regression (if multiple features), univariate/multivariate regression (if one/several quatities to predict), continuous flow of data or not, size of the training set, \ldots
    \item Select an error measurement
    \item Check that the assumptions that have been made are reasonable
\end{itemize}

\subsection{Get the data}
For reproducibility, it is best that everything is scripted, from the download of the data to the final product.

\begin{description}
  \item [Create virtual environment] \hfill
    \begin{python}
      python -m virtualenv .venv
    \end{python}
  \item [Download the data] \hfill
    \begin{python}
      import os
      import tarfile
      import urllib.request

      def fetch_data(url, data_path, data_file):
        os.makedirs(data_path, exist_ok=True)
        tgz_path = os.path.join(data_path, data_file)
        urllib.request.urlretrieve(url, tgz_path)
        data_tgz = tarfile.open(tgz_path)
        data_tgz.extractall(path=data_path)
        data_tgz.close()
    \end{python}

  \item [Load the data] \hfill
    \begin{python}
      import pandas as pd

      data = pd.read_csv(data_path)
    \end{python}

  \item [Quick info on the data] \hfill
    \begin{python}
      # pandas functions
      data.head()
      # Info about attributes and number of entries
      data.info()
      # Get the attribute_str data
      data["attribute_str"]
      # Counts occurences of the categories in category_str
      data["category_str"].value_counts()
      # count, mean, ...
      data.describe()
    \end{python}

    \item [Distribution of the features] \hfill
    \begin{python}
      import matplotlib.pyplot as plt

      data.hist(bins=50, figsize=(20,15))
      plt.show()
    \end{python}
\end{description}

Now we want to get a test set that we will not look at until we have selected the model and we a ready for release. The test gives an indication for the error the model will have on the new data it has never seen (the actual new data without label).\\

There are different ways to create the test set.
One must be careful in the way the train and test sets are created.
\begin{itemize}
  \item Indeed, we cannot just use random instances that change everytime we run the code.
    We must use a way that always uses the same instances, even if new data are added!
    Note that if no new data are added, the problem is simpler.
    This can be done by creating a unique idea for each instance and spliting by id.
  \item If a category is particularly important for the prediction, we need to keep the right propertions of this category in the train set and the test set.
  This is done using stratified sampling:
  \begin{python}
    from sklearn.model_selection import StratifiedShuffleSplit

    split = StratifiedShuffleSplit(n_splits=1, test_size=0.2,
                                   random_state=42)
    # It will split the data following the proportions of
    # the categories in category_str
    # Note that if it is not a category but a continuous value,
    # one can create a new features which is a bined version
    # of the continuous data
    for train_index, test_index in split.split(data,
                                        data["category_str"]):
      strat_train_set = data.loc[train_index]
      strat_test_set = data.loc[test_index]

    data = strat_train_set
  \end{python}
  Now the proportions are correct.
  If a category was created, delete it.
\end{itemize}

\subsection{Visualise the data}
It is now time to visualise the data and better understand them.

There are many plots and ways to investigate the data, here a few examples:
\begin{description}
  \item [Scatter Plot] \hfill
  \begin{python}
    data.plot(kind="scatter", x="cat_1", y="cat_2", alpha=alpha,
          s=cat_3, label=label, c=cat_4, cmap=plt.get_cmap("jet"),
          colorbar=True)
          plt.legend()
        \end{python}

  \item [Correlation] Correlation between pairs of attributes \hfill
    \begin{python}
      corr_matrix = data.corr()
      corr_matrix["label_var"].sort_values(ascending=False)
    \end{python}

  \item [Scattered matrix plot] Scatter of each attributes \hfill
    \begin{python}
      from pandas.plotting import scatter_matrix

      # Possibly too many attributes -> reduce
      attributes = [cat1, cat2, cat3]
      scatter_matrix(data[attributes], figsize=(12, 8))
    \end{python}

  \item [Combining attributes] Sometimes a combination of attributes is better than the attributes separately. \hfill
    \begin{python}
      data["new_var"] = data["var_1"] / data["var_2"]
      # And check the new correlation and hope it's better
      corr_matrix = data.corr()
      corr_matrix["label_var"].sort_values(ascending=False)
    \end{python}
\end{description}

\subsection{Preparing the data for machine learning}
Create functions to automatise the treatment of the data.
Preferably in a way that it is general and can be re-used later on.

    \subsubsection*{Data and labels}
      Start by separating the data and the labels for the train set.
      \begin{python}
        data = strat_train_set.drop("label_to_predict", axis=1)
        data_labels = strat_train_set["label_to_predict"].copy()
      \end{python}

    \subsubsection*{Missing Values}
      If there are missing values, there are different methods
      \begin{python}
        # Gets rid of the entities which lack the value in
        # cat_to_drop
        data.dropna(subset=["cat_to_drop"])
        # Get rid of the whole attribute
        data.drop("cat_to_drop", axis=1)
        # Replace by median value in the whole dataset
        median = data["cat_to_fill"].median()
        data["cat_to_fill"].fillna(median, inplace=True)
      \end{python}
      This can also be done using sklearn toolbox
      \begin{python}
        from sklearn.impute import SimpleImputer

        imputer = SimpleImputer(strategy="median")
        # need to remove categorical attributes
        data_numerical = data.drop("categorical_att", axis=1)
        imputer.fit(data_numerical)
        # Look at the medians:
        imputer.statistics_
        # Create a numpy array of transformed data with
        # filled values
        X = imputer.transform(data_numerical)
        data_treated = pd.DataFrame(X,
                columns=data_numerical.columns,
                index=data.data_numerical.index)
      \end{python}


    \subsubsection*{Categorical Attributes}
      \begin{itemize}
        \item replace them by integers from 0 to number of categories-1 and use that for training.
          \begin{python}
            from sklearn.preprocessing import OrdinalEncoder
            ordinal_encoder = OrdinalEncoder()
            data_cat = data[["categorical_attribute"]]
            data_cat_encoded = ordinal_encoder.fit_transform(data_cat)
          \end{python}
          There is a problem! The numbers have a relation between each other (bigger/smaller) and that property is not necessarily there in the categorical attributes.
        \item Instead create a new \emph{onehot} attribute for each of the original categories in the categorical\_attribute.
          \begin{python}
            from sklearn.preprocessing import OneHotEncoder
            cat_encoder = OneHotEncoder()
            # This will create the new attributes.
            # There will be as many new attributes as there were
            # categories in categorical_attribute.
            data_cat_1hot = cat_encoder.fit_transform(data_cat)
          \end{python}
      \end{itemize}

    \subsubsection*{Custom Transformers}
      Create own transformers that have \emph{fit} and \emph{fit\_transform}. This is usefull when creating a pipeline. It can be done by create a new class.\\
      If it inherits from TransformerMixin, \emph{fit\_transform()} gets created automatically.
      If it also inherits BaseEstimator, there are two more methods: \emph{get\_params()} and \emph{set\_params()}

      \begin{python}
        from sklearn.base import BaseEstimator, TransformerMixin
        class CombinedAttributesAdder(BaseEstimator, TransformMixin):
        def fit(self, X, y=None):
          # Fit the data X i.e. get the values out of the data
          # and save what must be saved
          return self
          def transform(self, X, y=None):
          # apply to the data the fit by using the saved values
          return

          # Which can be used:
          attr_adder = CombinedAttributesAdder()
          data_extra_attribs = attr_adder.fit_transform(data.values)
      \end{python}

    \subsubsection*{Feature Scaling}
    The range of values between the fields can vary a lot.\\
    The algorithm learns better with same range of values for each attribute
    There are two typical metods: min-max scaling and standardization
    \begin{python}
      from sklearn.preprocessing import MinMaxScaler,
                                    StandardScaler

      scaler = MinMaxScaler()   # or StandardScaler()
      data_scaled = scaler.fit_transform(data)
    \end{python}

    \subsubsection*{Transformation pipeline}
    To simplify the consecutive transformation of the data.\\
    Numerical pipeline:
    \begin{python}
      from sklearn.pipeline import Pipeline
      num_pipeline = Pipeline([
                  ('imputer', SimpleImputer(strategy="median")),
                  ('attribs_adder', CombinedAttributesAdder()),
                  ('std_scaler', StandardScaler())
                  ])
      data_tr = num_pipeline.fit_transform(data)
    \end{python}
    If numerical and categorical attributes:
    \begin{python}
      from sklearn.compose import ColumnTransformer
      num_attribs = list(data_numerical)
      cat_attribs = ["cat_attributes"]
      full_pipeline = ColumnTransformer([
            ("num", num_pipeline, num_attribs),
            ("cat", OneHotEncoder(), cat_attribs)
            ])
      data_prepared = full_pipeline.fit_transform(data)
    ?\end{python}

\subsection{Select and Train a model}
\subsubsection*{Training and evaluating on the training set}
    For example, it is easy to train a model:
    \begin{python}
        from sklearn.linear_model import LinearRegression

        lin_reg = LinearRegression()
        lin_reg.fit(data_prepared, data_labels)
    \end{python}

    It can then be tried on the training data:
    \begin{python}
        # Start by transforming the data through the pipeline!
        train_data_prepared = full_pipeline.transform(train_data)
        print("Predictions:", lin_reg.predict(train_data_prepared))
        print("Labels:", list(train_data_labels))
    \end{python}

    Error measure using RMSE:
    \begin{python}
        from sklearn.metrics import mean_squared_error
        data_predictions = lin_reg.predict(train_data_prepared)
        lin_mse = mean_squared_error(train_data_labels)
        lin_rmse = np.sqrt(lin_mse)
        print(lin_rmse)
    \end{python}

    This can give an idea if the model overfitted or underfitted the data during training.
    If underfitting, 3 solutions:
    \begin{itemize}
        \item Chose a more powerful model
        \item Feed the model better features
        \item Reduce the constraints on the model (remove regularization)
    \end{itemize}
    If overfitting, solutions:
    \begin{item}
        \item Simplify the model
        \item Constrain the model (regularize it)
        \item Get more data
    \end{item}

    More complex model:
    \begin{python}
        from sklearn.tree import DecisionTreeRegressor
        tree_reg = DecisionTreeRegressor()
        tree_reg.fit(train_data_prepared, train_data_labels)
    \end{python}
    Can be evaluated on training set using RMSE.
    If $rmse=0$ $\rightarrow$ overfitting!
    How can we be sure of overfitting? (\textbf{We cannot use the test set!! This is only to be used in the very end!})
    We split the train set into train set and validation set.

\subsubsection*{Better evaluation using cross-validation}
    A way to test whether the model is performing well would be to split the training set into a smaller training set and a validation set.

    The different models could be trained on the smaller training set and evaluated on the evaluation set.

    An alternative is to use \emph{K-fold cross-validation}:\\
    \begin{itemize}
        \item it splits the training set into 10 subsets (folds)
        \item then it trains the model 10 times, everytime using a different fold for the validation and the other 9 folds as the training data.
    \end{itemize}
    \begin{python}
        from sklearn.model_selectoin import cross_val_score

        scores = cross_val_score(model, train_data_prepared, train_data_labels,
                    scoring="neg_mean_squared_error", cv=10)
        model_rmse_scores = np.sqrt(-scores)

        def display_scores(scores):
            print("Scores: " scores)
            print("Mean: ", scores.mean())
            print("Standard deviation: ", scores.std())

        display_scores(model_rmse_scores)
    \end{python}

    Another model can be trained, e.g.\ \emph{RandomForestRegressor} (it trains many decision trees on subsets of features and averages their preidictions).
    The use of several models on top of each other: \emph{Ensemble Learning}
    \begin{python}
        from sklearn.ensemble import RandomForestRegressor

        model = RandomForestRegressor()
        model.fit(train_data_prepared, train_data_labels)
        scores = cross_val_score(model, train_data_prepared, train_data_labels,
                    scoring="neg_mean_squared_error", cv=10)
        model_rmse_scores = np.sqrt(-scores)
    \end{python}

    Before deeping into a model (tweaking hyperparameters), try several different and keep the best ones!
    Typically keep between 2 and 5 models.

    Save:
    \begin{itemize}
        \item models
        \item hyperparameters
        \item trained parameters
        \item cross-validation scores
        \item actuall predictions
    \end{itemize}
    This allows to easily compare the different models.
    \begin{python}
        import joblib

        joblib.dump(model, 'model.pkl')
        # model = joblib.load('model.pkl')
    \end{python}

    \subsection{Fine-tune your model}
    How to fine-tune:
    \subsubsection*{Grid Search}
    Goes through the hyperparameter space and train several models and evaluate.
    Once all are trained, one can get the best parameters (or just directly the best model)

    \begin{python}
        from sklearn.model_selection import GridSearchCV

        param_grid = [
            {'n_estimators': [3,10,30],
             'max_features': [2,4,6,8]
            },      # Here: 3x4=12 models to train
            {'bootstrap':[False],
             'n_estimators':[3,10],
             'max_features':[2,3,4]
            },      # here: 2x3=6 models to train with bootstrap on
        ]

        model = RandomForestRegressor()
        grid_search = GridSearchCV(mode, param_grid, cv=5,
                        scoring='neg_mean_squared_error',
                        return_train_score=True)
        grid_search.fit(train_data_prepared, train_data_labels)
    \end{python}
    The grid search will do the grid search for each of the dictionaries in \verb;param_grid;.

    The results from each:
    \begin{python}
        cvres = grid_search.cv_results_
        for mean_score, params in zip(["mean_test_score"], cvres["params"]):
            print(np.sqrt(-mean_score, params))
    \end{python}

    Get the best:
    \begin{python}
        grid_search.best_params_
        grid_search.best_estimator_
    \end{python}

    Normal grid search is fine for a small parameter space, otherwise \emph{RandomizedSearchCV} woule be better.
    It tries random values of the hyperparameters.
    \begin{python}
        n_estimators = [int(x) for x in np.linspace(start=250, stop=300, num=2)]
        max_features = ['auto', 'sqrt']
        random_grid = {'n_estimators': n_estimators,
                       'max_features': max_features}
        model = RandomForestRegressor()
        model_random = RandomizedSearchCV(estimator=model,
                                    param_distributions=random_grid,
                                    n_iter=1000, cv=2, verbose=2,
                                    random_state=42, n_jobs=-1)
    \end{python}

    \subsubsection*{Ensemble methods}
    Combining the best models could perform better $\rightarrow$ Ensemble

    \subsubsection*{Analyze best models and their errors}
    Looking at the models gives good insights.

    \emph{RandomForestRegressor} can indicate the relative importance of the attributes:
    \begin{python}
        # If grid_search using RandomForestRegressor
        feature_importances = grid_search.best_estimator_.featue_importances_

        # shown next to the features name:
        extra_attribs = ["extra_attribs"]       # Created in the pipeline by combining the attributes
        # Extract the attributes created from the categorical attributes using OneHotEncoder
        cat_encoder = full_pipeline.named_transformers_["cat"]
        cat_one_hot_attribs = list(cat_encoder.categories_[0])
        # Concatenate all the attributes
        attributes = num_attribs + extra_attribs + cat_one_hot_attribs

        sorted(zip(feature_importances, attributes), reverse=True)
    \end{python}

    One should look at the specific errors made by the model to try to fix those, example:
    \begin{itemize}
        \item add extra features
        \item get rid of uninformative features
        \item cleaning up outliers
    \end{itemize}

    \subsubsection*{Evaluate your system on the test set}
        Time to try the model on the test set.
        \begin{itemize}
            \item Get the test data and labels.
            \item run the \verb;full_pipeline;
            \item call \verb;.transform();
        \end{itemize}

        \begin{python}
            final_model = grid_search.best_estimator_

            X_test = strat_test_set.drop("feature_to_predict", axis=1)
            y_test = strat_test_set["feature_to_predict"].copy()

            X_test_prepared = full_pipeline.tranform(X_test)
            final_predictions = final_model.predict(X_test_prepared)

            final_mse = mean_squared_error(y_test, final_predictions)
            final_rmse = np.sqrt(final_mse)
        \end{python}

        Can also look at the confidence interval:
        \begin{python}
            from scipy import stats
            confidence = 0.95
            squared_errors = (final_predictions - y_test) ** 2
            np.sqrt(stats.t.interval(confidence, len(squared_errors) - 1,
                                     loc=squared_errors.mean(),
                                     scale=stats.sem(squared_errors)))
        \end{python}
        If the results are worse than on the validation set, DO NOT FINETUNE THE HYPERPARAMETERS TO GET BETTER RESULTS!!

    \subsection{Launch}
    It is now time to launch, monitor and maintain the system!

    Once it is released:

    Write monitor code to check the live performance regularly and throw allerts if it drops.
    Drop in performance may be quick (something break) or slow (model "rot" over time because of the newer data beeing a bit different from the older ones).

    The monitoring downstream has to be adapted to the delivered product.
    $\rightarrow$ Automate the process as much as possible
    \begin{itemize}
        \item collect fresh data regularly and label it
        \item Write script to train the model and fine-tune the hyper-parameters (run e.g.\ everyday)
        \item Write a script that compares the new and the previous model on the updated test set and deploy the model to production if the performances have not decreased (if it did, why??)
    \end{itemize}

    Check the input data quality.

    Keep backups of every model!
    (And have what is necessary to easily move back to a previous model)

    Also keep backups of every version of the datasets (in case the newly added data have a lot of outliers).


\end{document}
