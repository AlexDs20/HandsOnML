% Specifies the type of document you have.
\documentclass{article}
\usepackage{array}
\usepackage{listings}
\usepackage{geometry}
\renewcommand{\arraystretch}{1.5}
\geometry{
  a4paper,
  total={140mm, 217mm}
}
%------------------------------

% Default fixed font does not support bold face
\DeclareFixedFont{\ttb}{T1}{txtt}{bx}{n}{10} % for bold
\DeclareFixedFont{\ttm}{T1}{txtt}{m}{n}{10}  % for normal

% Custom colors
\usepackage{color}
\definecolor{deepblue}{rgb}{0,0,0.5}
\definecolor{deepred}{rgb}{0.6,0,0}
\definecolor{deepgreen}{rgb}{0,0.5,0}

% Python style for highlighting
\newcommand\pythonstyle{\lstset{
language=Python,
basicstyle=\ttm,
otherkeywords={self},             % Add keywords here
keywordstyle=\ttb\color{deepblue},
emph={MyClass,__init__},          % Custom highlighting
emphstyle=\ttb\color{deepred},    % Custom highlighting style
stringstyle=\color{deepgreen},
showstringspaces=false            %
}}

% Python environment
\lstnewenvironment{python}[1][]
{
\pythonstyle
\lstset{#1}
}
{}

% Python for external files
\newcommand\pythonexternal[2][]{{
\pythonstyle
\lstinputlisting[#1]{#2}}}

% Python for inline
\newcommand\pythoninline[1]{{\pythonstyle\lstinline!#1!}}


%------------------------------

\title{Notes on:\\ "Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow"}
\author{Alexandre De~Spiegeleer}

\begin{document}
% Define some things for hte python codes


\maketitle
\newpage

%----------------------------------------
\section{The machine learning landscape}
%----------------------------------------
ML is nothing more than fitting a model to the known data!
\subsection{Sturcture of a project}
\begin{enumerate}
    \item study the problem
    \item Train the ML algo (with lots of data)
    \item Solution
    \item Check the solution and better understand the problem
    \item[$\rightarrow$] try to improve model from the observed possible problems
\end{enumerate}

Examples:
\begin{itemize}
    \item Detection of signals in images using CNNs
    \item Classifying articles: NLP
\end{itemize}


\subsection{Types of ML}
Different categories of ML algorithms:

\begin{tabular}{p{0.20\textwidth}p{0.40\textwidth}p{0.4\textwidth}}
  \textbf{Properties} & \textbf{Descrition} & \textbf{Examples} \\
  \hline
  Supervised &  Need laballed training data & kNN, Linear Reg., Logistic Reg., SVM, Decision Tree, Random Forest, NN\ldots \\
  Unsupervised & No labelled data, the algorithm puts similar data together & Clustering: k-means, DBSCAN, HCA. Dimensional reduction: PCA, kernel PCA, LLE, t-SNE. Anomaly detection: One-class SVM, Isolation Forest\\
  Semi-Supervised & Both labelled and not labelled data & Deepd belief network, restricted Boltzman machine \\
  Reinforcment Learning & In the learning process give rewards or penalites depending on the action done & \\
  \hline\hline
  Batch Learning & Trains using ALL data $\rightarrow$ If needs to retrain, need to train on all the data again & \\
  Online Learning & Train by mini-batches: Train incrementally and can start again from a previous minibatch run & \\
  \hline\hline
  Instance Based & On new data, check distance to known data and assign same output &\\
  Model-based & Use the data to make predictions / interpolates to new data & \\
\end{tabular}

\subsubsection{Unsupervised Learning}
\begin{description}
    \item[Feature Extraction] Combines related features into a better one (e.g. using PCA)
    \item[Anomaly Detection] Find anomalies in dataset(e.g.\ removing some outliers)
    \item[Association rule learning] Discover relationships in large datasets
\end{description}

\subsubsection{semi-supervised Learning}
Often much data but only a few are labelled
\begin{itemize}
  \item[$\rightarrow$] Often combinaison of supervised and unsupervised algo.
\end{itemize}

\subsubsection{Reinforcement Learning}
Trains an agent by giving rewards and penalties to obtained the best policy

\subsection{Challenges}
\subsubsection{data}
\begin{description}
  \item[Lack of data] ML requires lots of data (mininmum thousands of examples).\\
                      Note that models performances increase with number of data.\\
                      $\rightarrow$ Choice to work on model or gather more data!

  \item[Non representative data] Data used for training must include similar data to those for predictions. (Poor extrapolation capacity)

  \item[Sampling Bias] non representative data because the sampling is flawed.\\
          Bias can appear if the proportion of data in different classes are not representative.

  \item[Poor data quality] if errors, outliers and noise in data

  \item[Irrelevant features] There should be enough relevant features and not much crap.
\end{description}


\subsubsection{Fitting}
\begin{description}
  \item[Overfitting] The model fits too well the training data. Occurs because model too complex compared to data $\rightarrow$ lose predictability.
  \item[Underfitting] Model too simple compared to the data to be represented e.g.\ Linear fit on a non-linear problem
\end{description}

Solutions to overfitting:
\begin{itemize}
  \item \textit{Regularization}\\
    $\rightarrow$ Making the model simpler to avoid by adjusting hyperparameter
  \item \textit{Hyperparameter}\\
    $\rightarrow$ Parameters of the learning algorithm that dictates the amount of regularization.
\end{itemize}

Solutions to underfitting
\begin{itemize}
  \item Select a better model
  \item Have better features
  \item reduce the constraints on the model (e.g.\ hyperparameters)
\end{itemize}

\subsection{Evaluating performance}

Split the data into training set (80\%) and testing set (20\%).
Evaluation on the test set gives an estimate of the error on unseen data.
When training multiple models on the training set and testing them on the test sets, our selection of the "best" model is which model best fits the test data set.
$\rightarrow$ Model cannot necessarily be good on new data.
Thus, keep a validation set.

Thus, there are 3 sets of data: Training, Validation and Test
\begin{enumerate}
  \item Train multiple models with different Hyperparameter on the training set
  \item Select the model that performs best on the validation set.
  \item Verify that the model is indeed good on the test set
  \item Deploy
\end{enumerate}

It is common to split the training set to train several models on sub-sets of the training set and train one final model on the whole training set.

If the validation set is too small $\rightarrow$ imprecise evaluation of performances.
The validation set cannot be too large either (compared to training set).\\
$\rightarrow$ use \emph{cross-validation}: it uses several small validation sets.
Each model is evaluated once per validation set.
You can then average the results of the model on the different smaller validation sets.

\subsection{Data mismatch}
If there are two types of data in the training set (e.g.\ not from the same source).
This may cause problem in the predictions.
How to know if overfitting or problem with the data?

Hold part of the training set (data from one of the source) and see how the model performs on that.
If it performs well $\rightarrow$ the error comes from the mismatched data
If it performs poorly $\rightarrow$ the error comes from the overfitting

If it is because of the data mismatched, is it possible to preprocess these to be more like the rest of the data?



%----------------------------------------
\newpage
\section{End-to-end machine learning project}
%----------------------------------------

Main steps of a machine learning project:
\begin{enumerate}
    \item Look at the big picture
    \item Get the data
    \item Discover and visualise the data
    \item Prepare the data for machine learning algorithms
    \item Select a model and train it
    \item Fine-tune the model
    \item Present the solution
    \item Launch, monitor and maintain the system
\end{enumerate}

\subsection{Look at the big picture}
Get info on the problem to be solved e.g.\
\begin{itemize}
    \item Objectives?
    \item input and output of the model? (i.e.\ features and regression/categories?)
    \item Overall project's pipeline
    \item Current status and precision $\rightarrow$ idea for the aimed accuracy
    \item Evaluate what the model needs: multiple regression (if multiple features), univariate/multivariate regression (if one/several quatities to predict), continuous flow of data or not, size of the training set, \ldots
    \item Select an error measurement
    \item Check that the assumptions that have been made are reasonable
\end{itemize}

\subsection{Get the data}
For reproducibility, it is best that everything is scripted, from the download of the data to the final product.

\begin{description}
  \item [Create virtual environment] \hfill
    \begin{python}
      python -m virtualenv .venv
    \end{python}
  \item [Download the data] \hfill
    \begin{python}
      import os
      import tarfile
      import urllib.request

      def fetch_data(url, data_path, data_file):
        os.makedirs(data_path, exist_ok=True)
        tgz_path = os.path.join(data_path, data_file)
        urllib.request.urlretrieve(url, tgz_path)
        data_tgz = tarfile.open(tgz_path)
        data_tgz.extractall(path=data_path)
        data_tgz.close()
    \end{python}

  \item [Load the data] \hfill
    \begin{python}
      import pandas as pd

      data = pd.read_csv(data_path)
    \end{python}

  \item [Quick info on the data] \hfill
    \begin{python}
      # pandas functions
      data.head()
      # Info about attributes and number of entries
      data.info()
      # Get the attribute_str data
      data["attribute_str"]
      # Counts occurences of the categories in category_str
      data["category_str"].value_counts()
      # count, mean, ...
      data.describe()
    \end{python}

    \item [Distribution of the features] \hfill
    \begin{python}
      import matplotlib.pyplot as plt

      data.hist(bins=50, figsize=(20,15))
      plt.show()
    \end{python}
\end{description}

Now we want to get a test set that we will not look at until we have selected the model and we a ready for release. The test gives an indication for the error the model will have on the new data it has never seen (the actual new data without label).\\

There are different ways to create the test set.
One must be careful in the way the train and test sets are created.
\begin{itemize}
  \item Indeed, we cannot just use random instances that change everytime we run the code.
    We must use a way that always uses the same instances, even if new data are added!
    Note that if no new data are added, the problem is simpler.
    This can be done by creating a unique idea for each instance and spliting by id.
  \item If a category is particularly important for the prediction, we need to keep the right propertions of this category in the train set and the test set.
  This is done using stratified sampling:
  \begin{python}
    from sklearn.model_selection import StratifiedShuffleSplit

    split = StratifiedShuffleSplit(n_splits=1, test_size=0.2,
                                   random_state=42)
    # It will split the data following the proportions of
    # the categories in category_str
    # Note that if it is not a category but a continuous value,
    # one can create a new features which is a bined version
    # of the continuous data
    for train_index, test_index in split.split(data,
                                        data["category_str"]):
      strat_train_set = data.loc[train_index]
      strat_test_set = data.loc[test_index]

    data = strat_train_set
  \end{python}
  Now the proportions are correct.
  If a category was created, delete it.
\end{itemize}

\subsection{Visualise the data}
It is now time to visualise the data and better understand them.

There are many plots and ways to investigate the data, here a few examples:
\begin{description}
  \item [Scatter Plot] \hfill
  \begin{python}
    data.plot(kind="scatter", x="cat_1", y="cat_2", alpha=alpha,
          s=cat_3, label=label, c=cat_4, cmap=plt.get_cmap("jet"),
          colorbar=True)
          plt.legend()
        \end{python}

  \item [Correlation] Correlation between pairs of attributes \hfill
    \begin{python}
      corr_matrix = data.corr()
      corr_matrix["label_var"].sort_values(ascending=False)
    \end{python}

  \item [Scattered matrix plot] Scatter of each attributes \hfill
    \begin{python}
      from pandas.plotting import scatter_matrix

      # Possibly too many attributes -> reduce
      attributes = [cat1, cat2, cat3]
      scatter_matrix(data[attributes], figsize=(12, 8))
    \end{python}

  \item [Combining attributes] Sometimes a combination of attributes is better than the attributes separately. \hfill
    \begin{python}
      data["new_var"] = data["var_1"] / data["var_2"]
      # And check the new correlation and hope it's better
      corr_matrix = data.corr()
      corr_matrix["label_var"].sort_values(ascending=False)
    \end{python}
\end{description}

\subsection{Preparing the data for machine learning}
Create functions to automatise the treatment of the data.
Preferably in a way that it is general and can be re-used later on.

    \subsubsection*{Data and labels}
      Start by separating the data and the labels for the train set.
      \begin{python}
        data = strat_train_set.drop("label_to_predict", axis=1)
        data_labels = strat_train_set["label_to_predict"].copy()
      \end{python}

    \subsubsection*{Missing Values}
      If there are missing values, there are different methods
      \begin{python}
        # Gets rid of the entities which lack the value in
        # cat_to_drop
        data.dropna(subset=["cat_to_drop"])
        # Get rid of the whole attribute
        data.drop("cat_to_drop", axis=1)
        # Replace by median value in the whole dataset
        median = data["cat_to_fill"].median()
        data["cat_to_fill"].fillna(median, inplace=True)
      \end{python}
      This can also be done using sklearn toolbox
      \begin{python}
        from sklearn.impute import SimpleImputer

        imputer = SimpleImputer(strategy="median")
        # need to remove categorical attributes
        data_numerical = data.drop("categorical_att", axis=1)
        imputer.fit(data_numerical)
        # Look at the medians:
        imputer.statistics_
        # Create a numpy array of transformed data with
        # filled values
        X = imputer.transform(data_numerical)
        data_treated = pd.DataFrame(X,
                columns=data_numerical.columns,
                index=data.data_numerical.index)
      \end{python}


    \subsubsection*{Categorical Attributes}
      \begin{itemize}
        \item replace them by integers from 0 to number of categories-1 and use that for training.
          \begin{python}
            from sklearn.preprocessing import OrdinalEncoder
            ordinal_encoder = OrdinalEncoder()
            data_cat = data[["categorical_attribute"]]
            data_cat_encoded = ordinal_encoder.fit_transform(data_cat)
          \end{python}
          There is a problem! The numbers have a relation between each other (bigger/smaller) and that property is not necessarily there in the categorical attributes.
        \item Instead create a new \emph{onehot} attribute for each of the original categories in the categorical\_attribute.
          \begin{python}
            from sklearn.preprocessing import OneHotEncoder
            cat_encoder = OneHotEncoder()
            # This will create the new attributes.
            # There will be as many new attributes as there were
            # categories in categorical_attribute.
            data_cat_1hot = cat_encoder.fit_transform(data_cat)
          \end{python}
      \end{itemize}

    \subsubsection*{Custom Transformers}
      Create own transformers that have \emph{fit} and \emph{fit\_transform}. This is usefull when creating a pipeline. It can be done by create a new class.\\
      If it inherits from TransformerMixin, \emph{fit\_transform()} gets created automatically.
      If it also inherits BaseEstimator, there are two more methods: \emph{get\_params()} and \emph{set\_params()}

      \begin{python}
        from sklearn.base import BaseEstimator, TransformerMixin
        class CombinedAttributesAdder(BaseEstimator, TransformMixin):
        def fit(self, X, y=None):
          # Fit the data X i.e. get the values out of the data
          # and save what must be saved
          return self
          def transform(self, X, y=None):
          # apply to the data the fit by using the saved values
          return

          # Which can be used:
          attr_adder = CombinedAttributesAdder()
          data_extra_attribs = attr_adder.fit_transform(data.values)
      \end{python}

    \subsubsection*{Feature Scaling}
    The range of values between the fields can vary a lot.\\
    The algorithm learns better with same range of values for each attribute
    There are two typical metods: min-max scaling and standardization
    \begin{python}
      from sklearn.preprocessing import MinMaxScaler,
                                    StandardScaler

      scaler = MinMaxScaler()   # or StandardScaler()
      data_scaled = scaler.fit_transform(data)
    \end{python}

    \subsubsection*{Transformation pipeline}
    To simplify the consecutive transformation of the data.\\
    Numerical pipeline:
    \begin{python}
      from sklearn.pipeline import Pipeline
      num_pipeline = Pipeline([
                  ('imputer', SimpleImputer(strategy="median")),
                  ('attribs_adder', CombinedAttributesAdder()),
                  ('std_scaler', StandardScaler())
                  ])
      data_tr = num_pipeline.fit_transform(data)
    \end{python}
    If numerical and categorical attributes:
    \begin{python}
      from sklearn.compose import ColumnTransformer
      num_attribs = list(data_numerical)
      cat_attribs = ["cat_attributes"]
      full_pipeline = ColumnTransformer([
            ("num", num_pipeline, num_attribs),
            ("cat", OneHotEncoder(), cat_attribs)
            ])
      data_prepared = full_pipeline.fit_transform(data)
    \end{python}

\subsection{Select and Train a model}


\end{document}
