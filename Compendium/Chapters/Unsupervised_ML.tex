\section{Unsupervised ML}

\subsection{Clustering}

%------------------
\subsubsection{K-means}
%------------------
K-means look for a centered point for each cluster: \verb;centroid;

\begin{python}
    from sklearn.cluster import KMeans
    k = 5
    kmeans = KMeans(n_clusters=k)
    X, y = load_data()
    y_pred = kmeans.fit_predict(X)
\end{python}

\begin{description}
    \item[Hard clustering] \hfill\\
            Assigning each instance to a single cluster using distance to closest centroid
    \item[Soft clustering] \hfill\\
            Give a score to the instance for each centroid:
            \begin{itemize}
                \item Distance between instance and centroid
                \item Similarity: e.g.\ Gaussian Radial Basis
            \end{itemize}
\end{description}

How it works:
\begin{enumerate}
    \item Randomly choose centroid
    \item label instances
    \item update centroid
    \item Repeat 2-3
\end{enumerate}

The solution may not be optimal because of the initialization of the centroid.
\begin{itemize}
    \item If there is an idea of where they should be: use that as initial centroid
    \item Compute several models with many random init centroid and measures which model is best using "inertia":\\
            mean square distance to the closest centroid
    \item KMeans++ : better centroid initialization (used by KMeans class in sklearn)
\end{itemize}

\begin{description}
    \item[Preproc.]: \hfill\\
        Scale the input features before using K-Means \arrow improves performances
    \item[Faster or bigger data sets]: \hfill\\
        For datasets that do not fit in memory or for better speed: training using mini-batches and \verb;memmap;
        \begin{python}
            import numpy as np
            X_mm = np.memmap(file, dtype="float32", mode="readonly", shape=(m,n))
        \end{python}
        using minibatch usually gives worse inertia.
    \item[Number of clusters]: how to find the optimal number of clusters? \hfill\\
        The result may be bad if wrongly choose the number of clusters!\\
        One way is to compute the inertia for increasing number of cluster and identify the "elbow".\\
        Alternatively:\\
        \begin{itemize}
            \item \verb;Silhouette score;: mean silhouette coefficient ($b-a/\mathrm{max}(a,b)$) over all instances,
                with $a$: the mean distance to the other instances in the same clusters and $b$: mean distance to the
                instances of the next closest cluster.\\
                Vary between -1 and 1:
                \begin{itemize}
                    \item[+1] : instance is well inside its own cluster
                    \item[near 0] : close to cluster boundary
                    \item[-1] : instance assigned to the wrong cluster
                \end{itemize}
        \end{itemize}
        \begin{python}
        \end{python}
    \item[Limitations]: \hfill\\
        \begin{itemize}
            \item Need to be ran several times for different number of clusters
            \item Does not behave well if clusters have varying size, different densities, non-spherical shapes\\
                \arrow use Gaussian mixture models
        \end{itemize}
    \item[Semi-Supervised Learning]: \hfill\\
        If there are many unlabeled instances and few labeled instances:\\
        \begin{itemize}
            \item use clustering with k clusters on all the training dataset
            \item find the training instances closest to the centroid and use them as reference instances
            \item These reference instances are labelled by hand
            \item The labels can be extended to the whole or only the closest part of the cluster
            \item Train the model on the now labeled dataset
        \end{itemize}
        To improve:
        \begin{itemize}
            \item Manually setting the instances for which the model is most uncertain
        \end{itemize}
\end{description}

%------------------
\subsubsection{DBSCAN}
%------------------

Based on local density estimation.\\
It allows to identify clusters of arbitrary shapes as continuous regions of high density.
\begin{itemize}
    \item For each instance, counts how many instances are close to it: $\epsilon$\verb;-neighborhood;
    \item if at least \verb;min_samples; in $\epsilon$\verb;-neighborhood; \arrow it is a core instance (located in
        dense region)
    \item all instances around a core instance form a cluster
    \item instances that does not have a core instance \arrow anomaly
\end{itemize}

Works well if
\begin{itemize}
    \item clusters are dense enough
    \item clusters are separated by low-density regions
\end{itemize}

\begin{python}
    from sklearn.cluster import DBSCAN
    X, y = load_data()
    dbscan = DBSCAN(eps=0.2, min_samples=5)
    dbscan.fit(X)
\end{python}
Note that the choice of $\epsilon$ and \verb;min_samples; have drastic effect.

DBSCAN
\begin{itemize}
    \item no predict method \Arrow use another method (e.g.\ KNN)
    \item cannot identify clusters of different densities \arrow use HDBSCAN (scan through epsilon for best stability)
\end{itemize}

%------------------
\subsubsection{Other Clustering Algorithms}
%------------------
\begin{description}
    \item[Agglomerative clustering] \hfill\\
        Connects smaller clusters to make larger ones and can choose the cluster scale
    \item[BIRCH] \hfill\\
        Designed for large datasets, if number of features $<20$.
    \item[Mean-Shift] \hfill\\
        Creates circles that are shifted to find local density maximum.\\
        Tends to split a cluster of varying density into subclusters.\\
        Does not scale well.
    \item[Affinity propagation] \hfill\\
        Creates clusters by finding the instances that are most alike.\\
        Does not scale well.
    \item[Spectral clustering] \hfill\\
        reduce dimensionality of similarity matrix between the instances.\\
        Use a clustering algorithm in the lower dimensional space.\\
        Does not scale well.
\end{description}


%------------------
\subsubsection{Gaussian Mixtures}
%------------------
\begin{itemize}
    \item Assumes that the instances come from a mixture of several Gaussian distributions
    \items \verb;GaussianMixture;: must know the number of gaussians in advance.
        \begin{itemize}
            \item uses \verb;Expectation-Maximization; (EM) algorithm
            \item Can end up converging to poor solutions
            \item[\arrow] needs to be run several times and keep best solution
        \end{itemize}
    \item Gaussian mixture is a generative model, meaning that you can sample new instances from it
    \item It can be used to detect anomalies for fraud detection or \textit{removing outliers} from a dataset before
        training another model:
        \begin{itemize}
            \item instances located in low-density region can be considered anomaly.
            \item[\arrow] must define density threshold such that x\% of the data are below the threshold (check that
                these are indeed outliers or adjust the threshold)
        \end{itemize}
    \item Can be used for novelty detection if the dataset is clean (uncontaminated by outliers (anomaly detection does
        not assume this))
    \item find number of clusters:
        \begin{itemize}
            \item Find the model that minimizes a theretical information criterion, e.g.\ Bayesian information criterion
                or Akaike information criterion
        \end{itemize}
\end{itemize}

\subsection{Bayesian Gaussian Mixture Models}
Instead of manually looking for the number of clusters, \arrow use \verb;BayesianGaussianMixture;.
\begin{itemize}
    \item Can give weights close to zero for unnecessary clusters
    \item sen \verb;n_components; to a value larger than the optimal number of clusters
\end{itemize}
